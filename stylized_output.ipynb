{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN54Ph8ou1V3GtCb/Qby5F1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aneez-hassan/PRODIGY_GA_05/blob/main/stylized_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Style Transfer with Automatic Example Images\n",
        "# Run this in Google Colab\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# Function to load image from URL and transform it\n",
        "def load_image_from_url(url, max_size=400, shape=None):\n",
        "    response = requests.get(url)\n",
        "    image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "    if max(image.size) > max_size:\n",
        "        size = max_size\n",
        "    else:\n",
        "        size = max(image.size)\n",
        "\n",
        "    if shape is not None:\n",
        "        size = shape\n",
        "\n",
        "    in_transform = transforms.Compose([\n",
        "        transforms.Resize(size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406),  # Imagenet mean\n",
        "                             (0.229, 0.224, 0.225))  # Imagenet std\n",
        "    ])\n",
        "\n",
        "    # Apply transform and add batch dimension\n",
        "    image = in_transform(image).unsqueeze(0)\n",
        "    return image\n",
        "\n",
        "# Function to convert tensor to image for display\n",
        "def im_convert(tensor):\n",
        "    image = tensor.to(\"cpu\").clone().detach()\n",
        "    image = image.squeeze(0)\n",
        "    image = image * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n",
        "    image = image.clamp(0,1)\n",
        "    image = image.permute(1, 2, 0).numpy()\n",
        "    return image\n",
        "\n",
        "# URLs for example content and style images\n",
        "content_url = \"https://pytorch.org/tutorials/_static/img/neural-style/picasso.jpg\"\n",
        "style_url = \"https://pytorch.org/tutorials/_static/img/neural-style/dancing.jpg\"\n",
        "\n",
        "# Load images\n",
        "content = load_image_from_url(content_url)\n",
        "style = load_image_from_url(style_url, shape=[content.size(2), content.size(3)])\n",
        "\n",
        "# Display content and style images\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
        "ax1.imshow(im_convert(content))\n",
        "ax1.set_title('Content Image')\n",
        "ax1.axis('off')\n",
        "ax2.imshow(im_convert(style))\n",
        "ax2.set_title('Style Image')\n",
        "ax2.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Load VGG19 model pretrained on ImageNet\n",
        "vgg = models.vgg19(pretrained=True).features\n",
        "\n",
        "# Freeze parameters since we only want to optimize the target image\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vgg.to(device)\n",
        "\n",
        "# Define layers to use for content and style\n",
        "content_layers = ['21']  # relu4_2\n",
        "style_layers = ['0', '5', '10', '19', '28']  # relu1_1, relu2_1, relu3_1, relu4_1, relu5_1\n",
        "\n",
        "# Function to get features from layers\n",
        "def get_features(image, model, layers=None):\n",
        "    features = {}\n",
        "    x = image.to(device)\n",
        "    for name, layer in model._modules.items():\n",
        "        x = layer(x)\n",
        "        if name in layers:\n",
        "            features[name] = x\n",
        "    return features\n",
        "\n",
        "# Function to calculate Gram Matrix for style representation\n",
        "def gram_matrix(tensor):\n",
        "    batch, d, h, w = tensor.size()\n",
        "    tensor = tensor.view(d, h * w)\n",
        "    gram = torch.mm(tensor, tensor.t())\n",
        "    return gram\n",
        "\n",
        "# Get content and style features\n",
        "content_features = get_features(content, vgg, content_layers)\n",
        "style_features = get_features(style, vgg, style_layers)\n",
        "\n",
        "# Calculate style Gram matrices\n",
        "style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
        "\n",
        "# Create a target image and initialize it with content image\n",
        "target = content.clone().requires_grad_(True).to(device)\n",
        "\n",
        "# Define weights for style layers and content\n",
        "style_weights = {'0': 1.0,\n",
        "                 '5': 0.75,\n",
        "                 '10': 0.2,\n",
        "                 '19': 0.2,\n",
        "                 '28': 0.2}\n",
        "\n",
        "content_weight = 1e4\n",
        "style_weight = 1e2\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam([target], lr=0.003)\n",
        "\n",
        "# Run style transfer\n",
        "steps = 300  # Increase for better quality\n",
        "\n",
        "for step in range(steps):\n",
        "    target_features = get_features(target, vgg, content_layers + style_layers)\n",
        "\n",
        "    # Content loss\n",
        "    content_loss = torch.mean((target_features[content_layers[0]] - content_features[content_layers[0]])**2)\n",
        "\n",
        "    # Style loss\n",
        "    style_loss = 0\n",
        "    for layer in style_layers:\n",
        "        target_feature = target_features[layer]\n",
        "        target_gram = gram_matrix(target_feature)\n",
        "        style_gram = style_grams[layer]\n",
        "        layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
        "        style_loss += layer_style_loss / (target_feature.numel())\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = content_weight * content_loss + style_weight * style_loss\n",
        "\n",
        "    # Update target image\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss every 50 steps\n",
        "    if step % 50 == 0:\n",
        "        print(f\"Step {step}, Total loss: {total_loss.item():.4f}\")\n",
        "\n",
        "# Display final stylized image\n",
        "final_img = im_convert(target)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(final_img)\n",
        "plt.title(\"Stylized Image\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dd4AtjgydxfW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}